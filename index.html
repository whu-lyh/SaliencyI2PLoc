
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>SaliencyI2PLoc: saliency-guided image-point cloud localization using contrastive learning</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <!-- <link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,800italic,400,700,800' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="css/project.css" media="screen" />
    <link rel="stylesheet" type="text/css" media="screen" href="css/iconize.css" />
    <script src="js/google-code-prettify/prettify.js"></script> -->
  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <h2>SaliencyI2PLoc: saliency-guided image-point cloud localization <br> using contrastive learning
            </h2>
            <h4 style="color:#5a6268;">Underview</h4>
            <hr>
            <h6>
                <a href="https://whu-lyh.github.io/" target="_blank">Yuhao Li</a><sup>1</sup>,
                <a href="https://kafeiyin00.github.io/" target="_blank">Jianping Li</a><sup>2&dagger;</sup>,
                <a href="https://dongzhenwhu.github.io/" target="_blank">Zhen Dong</a><sup>1&dagger;</sup>,
                <a href="https://scholar.google.com/citations?user=yvQpME4AAAAJ&hl=zh-CN" target="_blank">Yuan Wang</a><sup>3</sup>,
                <a href="https://3s.whu.edu.cn/ybs/index.htm" target="_blank">Bisheng Yang</a><sup>1</sup></h6>
            <p>
                <sup>1</sup>State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University &nbsp;&nbsp;
                <br>
                <sup>2</sup>Nanyang Technological University &nbsp;&nbsp;&nbsp;&nbsp;
                <sup>3</sup>Jiangxi Normal University &nbsp;&nbsp;&nbsp;&nbsp;
                <br>
                <sup>&dagger;</sup>Corresponding authors. &nbsp;&nbsp;
            </p>

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Paper</a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/whu-lyh/SaliencyI2PLoc" role="button"  target="_blank">
                    <i class="fa fa-github-alt"></i> Code </a> </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- abstract -->
  <section class="Task teaser">
    <div class="container is-max-desktop">
      <div class="teaser-body">
        <p class="text-center">
          <strong>SaliencyI2PLoc</strong> aims to locate a image's position at a given reference point cloud map.
        </p>
        <div align=center>
          <img src="video/task_teaser.png" width="80%" height="80%">
        </div>
      </div>
    </div>
  </section>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
              <br>
            <hr style="margin-top:0px">

          <p class="text-justify">
            Image to point cloud global localization is crucial for robot navigation in GNSS-denied environments and has become increasingly important for multi-robot map fusion and urban asset management. The modality gap between images and point clouds poses significant challenges for cross-modality fusion. Current cross-modality global localization solutions either require modality unification, which leads to information loss, or rely on engineered training schemes to encode multi-modality features, which often lack feature alignment and relation consistency. To address these limitations, we propose, <strong>SaliencyI2PLoc</strong>, a novel contrastive learning based architecture that fuses the saliency map into feature aggregation and maintains the feature relation consistency on multi-manifold spaces. To alleviate the pre-process of data mining, the contrastive learning framework is applied which efficiently achieves cross-modality feature mapping. The context saliency-guided local feature aggregation module is designed, which fully leverages the contribution of the stationary information in the scene generating a more representative global feature. Furthermore, to enhance the cross-modality feature alignment during contrastive learning, the consistency of relative relationships between samples in different manifold spaces is also taken into account. Experiments conducted on urban and highway scenario datasets demonstrate the effectiveness and robustness of our method. Specifically, our method achieves a Recall@1 of 78.92\% and a Recall@1\% of 97.59\% on the urban scenario evaluation dataset, showing an improvement of approximately 33.11\% and 22\%, compared to the baseline method. This demonstrates that our architecture efficiently fuses images and point clouds and represents a significant step forward in cross-modality global localization.
          </p>
        <div align=center>
          <img src="video/methodology-multi-manifold-loss.png" width="100%" height="100%">
        </div>
        <p class="text-center">
          The pipeline of our methodology.
        </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h2>Results on the evaluation dataset built on the KITTI-360 dataset</h2>
            <hr style="margin-top:0px">
            <iframe src="//player.bilibili.com/player.html?isOutside=true&aid=112643989834582&bvid=BV17xgTevE4N&cid=500001588287226&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true">
            </iframe>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h2>Visualization of Top-K results</h2>
            <hr style="margin-top:0px">
            <div align=center>
              <img src="video/retrieval_results_visualization.png" width="80%" height="80%">
            </div>
          </p>
        </div>
      </div>
    </div>
  </section>
  <br>
  
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h2>Cluster assignments at urban scenario datasets</h2>
            <hr style="margin-top:0px">
            <div align=center>
              <img src="video/cluster_assignment_urban.png" width="80%" height="80%">
            </div>
            <p class="text-justify">
                The VLAD cluster assignment of the query images and the Top-1 point cloud from the database. The auxiliary point clouds/images are listed for better visualization. The referenced and Top-1 point clouds are rendered by the relative height, and whole point clouds are viewed from the bird eye view. The same color of patches in the cluster assignment subplots indicates the same cluster assigned.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h2>Saliency maps</h2>
            <hr style="margin-top:0px">
            <div align=center>
              <img src="video/saliency_maps_changes.png" width="60%" height="60%">
            </div>
            <p class="text-justify">
              The visualization of saliency maps of the query images. During training, we notice that the saliency map shifts to the scene layout and stationary buildings.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h2>More results</h2>
            <hr style="margin-top:0px">
            <div align=center>
              <img src="video/tsne_compare.png" width="60%" height="60%">
            </div>
            <p class="text-justify">
                The feature visualization by t-sne. The data index is rendered from blue to red in descending order. $\blacktriangle$ and $\cdot$ represent the feature from the query and database respectively.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>


  <!-- citing -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
                <code>@article{li2024saliencyi2ploc,
                  title={SaliencyI2PLoc: saliency-guided image-point cloud localization using contrastive learning},
                  author={Yuhao Li, Jianping Li, Zhen Dong, Yuan Wang and Bisheng Yang},
                  journal={Underreview},
                  year={2024}
                }</code>
              </pre>
          <hr>
      </div>
    </div>
  </div>

  <footer class="text-center" style="margin-bottom:10px">
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the website template.
  </footer>

</body>
</html>
